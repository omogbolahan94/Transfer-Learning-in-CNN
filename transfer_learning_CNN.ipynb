{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMjLzDBszG0aXVTXTk/FuvK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# TRANSFER LEARNING\n","\n","Modern deep neural networks are data-hungry. They require very large datasets, with millions of items, to reach their peak performance.\n","\n","Unfortunately, developing such large datasets from scratch for every use case of deep learning is very expensive and often not feasibe.\n","\n","**Transfer learning** is a technique that allows you take a neural network that has been already trained of one of these very large datasets, and tweak it slightly to adapt it to a new dataset.\n","\n","### Innovative CNN Architecture\n","Some innovative CNN architectures that accomplished significant breakthroughs in the ImageNet competitions:\n","* **AlexNet:**  ReLU activation function. AlexNet also used DropOut to prevent overfitting. It has the structure of a classical CNN, with a backbone made of convolution and Max Pooling followed by a flattening and a Multi-Layer Perceptron.\n","* **VGG:** The designers pioneered the use of many 3 by 3 convolutions instead of fewer larger kernels (for example, the first layer of AlexNet uses a 11 by 11 convolution). Therefore, he height and width of the feature maps decreases as we go deeper into the network, thanks to the Max Pooling layers, but the number of feature maps increases. The backbone is then followed by a flattening operation and a regular head made of a Multi-Layer Perceptron.\n","* **ResNet:** ResNet is a very important architecture that introduced a fundamental innovation: the skip connection.\n","\n","#### Global Average Pooling (GAP) Layer\n","A classic CNN has a first section comprised of several layers of convolutions and pooling, followed by a flattening and then one or more fully-connected layers (MLP).\n","\n","Fully-connected layers (head of a classic CNN) can only work with an input array of a specific size. Therefore, the vector produced by the flattening operation must have a specific number of elements, because it feeds into the fully-connected layers. Let's call this number of elements `H`. This means that the feature maps that are being flattened must have a specific size, so that `n_channels x height x width = H`. Since the height and width of the last feature maps are determined by the size of the input image, as it flows through the convolutional and the pooling layers, this constraint on the vector produced by the flattening operation translates to a constraint on the size of the input image. Therefore, for CNNs using flattening layers, the input size must be decided a priori when designing the architecture.\n","\n","For GAP, instead of taking the last feature maps (in the last convolution) and flattening them into a long vector, we take the **average of each feature map** and place them in a much shorter vector. This drastically reduces the dimensionality of the resulting vector, from `n_channels x height x width` to just `n_channels`. But also, more importantly, it makes the network adaptable to any input size because the flattening only depends on the feature maps of the last convolution and not the image size!\n","\n","**Note:** however that a network with GAP trained on a certain image size will not respond well to drastically different image sizes, even though it will output a result. So effectively the input size became a tunable parameter that can be changed without affecting the architecture of the CNN.\n","\n","Many modern architectures adopt the GAP layer.\n"],"metadata":{"id":"946dDcKUYDw2"}},{"cell_type":"markdown","source":["### Attention Layers\n","\n","#### Channel Attention: Squeeze and Excitation\n","*The term \"channel\" can refer to the channels in the input image (3 channels if RGB) but also to the number of feature maps are output from a layer.*\n","\n","**Channel attention** is a mechanism that a network can use to learn to pay more attention (i.e., to boost) feature maps that are useful for a specific example, and pay less attention to the others.\n","\n","This is accomplished by adding a sub-network (Squeeze and Excitation) that given the feature maps/channels assigns a scale to each input feature map. The feature maps with the largest scale are boosted.\n","\n","#### Self Attention: Transformers in Computer Vision\n"],"metadata":{"id":"a0D17MQAfcDp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzbMn2GeX1KW"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# GIT TRACKING"],"metadata":{"id":"FTGNvw_tjf7a"}},{"cell_type":"code","source":["!pip install python-dotenv --quiet"],"metadata":{"id":"9n1Wk1KTjmCD","executionInfo":{"status":"ok","timestamp":1751349380492,"user_tz":-60,"elapsed":10724,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from dotenv import load_dotenv\n","import os"],"metadata":{"id":"2UC9PwSJjmZQ","executionInfo":{"status":"ok","timestamp":1751349380508,"user_tz":-60,"elapsed":6,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["notebook_name = \"transfer_learning_CNN.ipynb\"\n","repo_name = \"Transfer-Learning-in-CNN\"\n","git_username = \"omogbolahan94\"\n","email = \"gabrielomogbolahan1@gmail.com\""],"metadata":{"id":"H3x27qfCjqX4","executionInfo":{"status":"ok","timestamp":1751349437282,"user_tz":-60,"elapsed":22,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def push_to_git(notebook_name, repo_name, commit_m, git_username, email):\n","  token_path = '/content/drive/MyDrive/Environment-Variable/variable.env'\n","  load_dotenv(dotenv_path=token_path)\n","  GITHUB_TOKEN = os.getenv('GIT_TOKEN')\n","\n","  USERNAME = f\"{git_username}\"\n","  REPO = f\"{repo_name}\"\n","\n","  # Authenticated URL\n","  remote_url = f\"https://{USERNAME}:{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO}.git\"\n","  if REPO not in os.listdir():\n","    !git clone {remote_url}\n","\n","  # copy notebook to the cloned CNN\n","  notebook_path = f\"/content/drive/My Drive/Colab Notebooks/{notebook_name}\"\n","  !cp '{notebook_path}' '/content/{REPO}/'\n","\n","  # ensure to be in the repository folder\n","  %cd '/content/{REPO}'\n","\n","  # copy the saved model into the cloned repository\n","  if \"cifar10_best_valid.pt\" not in os.listdir():\n","    if os.path.exists('/content/cifar10_best_valid.pt'):\n","      !cp /content/cifar10_best_valid.pt /content/{REPO}/\n","  if 'cifar10_network.pt' not in os.listdir():\n","    if os.path.exists('/content/cifar10_best_valid.pt'):\n","      !cp /content/cifar10_network.pt /content/{REPO}/\n","\n","  # Reconfigure Git\n","  !git config --global user.name '{USERNAME}'\n","  !git config --global user.email '{email}'\n","  !git remote set-url origin '{remote_url}'\n","\n","  print()\n","  !git add .\n","  !git commit -m '{commit_m}'\n","  !git push origin main\n","\n","  # change back to the content directory\n","  %cd '/content'"],"metadata":{"id":"D-ozNkW9j6HP","executionInfo":{"status":"ok","timestamp":1751349467976,"user_tz":-60,"elapsed":42,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["commit_m = \"Channel and self attention\""],"metadata":{"id":"kS27_viRkBlp","executionInfo":{"status":"ok","timestamp":1751349504800,"user_tz":-60,"elapsed":20,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["push_to_git(notebook_name, repo_name, commit_m, git_username, email)"],"metadata":{"id":"v2iNf-b1kKld","executionInfo":{"status":"ok","timestamp":1751349521386,"user_tz":-60,"elapsed":1467,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}},"outputId":"9bfca040-7647-4daa-be93-3b51a0cf3621","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Transfer-Learning-in-CNN'...\n","warning: You appear to have cloned an empty repository.\n","cp: cannot stat '/content/drive/My Drive/Colab Notebooks/transfer_learning_CNN.ipynb': No such file or directory\n","/content/Transfer-Learning-in-CNN\n","\n","On branch main\n","\n","Initial commit\n","\n","nothing to commit (create/copy files and use \"git add\" to track)\n","error: src refspec main does not match any\n","\u001b[31merror: failed to push some refs to 'https://github.com/omogbolahan94/Transfer-Learning-in-CNN.git'\n","\u001b[m/content\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pkzxAYnpkOTL"},"execution_count":null,"outputs":[]}]}